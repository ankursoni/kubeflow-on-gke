apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: basic-retrieval-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19, pipelines.kubeflow.org/pipeline_compilation_time: '2023-03-19T07:10:00.580563',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Basic retrieval pipeline",
      "inputs": [{"name": "gcs_bucket_name"}], "name": "Basic retrieval pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19}
spec:
  entrypoint: basic-retrieval-pipeline
  templates:
  - name: basic-retrieval-pipeline
    inputs:
      parameters:
      - {name: gcs_bucket_name}
    dag:
      tasks:
      - name: build-model
        template: build-model
        dependencies: [prepare-dataset]
        arguments:
          parameters:
          - {name: gcs_bucket_name, value: '{{inputs.parameters.gcs_bucket_name}}'}
          artifacts:
          - {name: prepare-dataset-movies, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-movies}}'}
          - {name: prepare-dataset-test, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-test}}'}
          - {name: prepare-dataset-train, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-train}}'}
          - {name: prepare-dataset-unique_movie_titles, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-unique_movie_titles}}'}
          - {name: prepare-dataset-unique_user_ids, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-unique_user_ids}}'}
      - {name: prepare-dataset, template: prepare-dataset}
  - name: build-model
    container:
      args: [--movies, /tmp/inputs/movies/data, --unique-user-ids, /tmp/inputs/unique_user_ids/data,
        --unique-movie-titles, /tmp/inputs/unique_movie_titles/data, --train, /tmp/inputs/train/data,
        --test, /tmp/inputs/test/data, --gcs-bucket-name, '{{inputs.parameters.gcs_bucket_name}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def build_model(
            movies_path,
            unique_user_ids_path,
            unique_movie_titles_path,
            train_path,
            test_path,
            gcs_bucket_name,
        ):
            """Function to build model."""
            import pickle
            from typing import Dict, Text

            import tensorflow as tf
            import tensorflow_recommenders as tfrs

            embedding_dimension = 32

            movies = tf.data.Dataset.load(movies_path, compression="GZIP")

            with open(file=unique_user_ids_path, mode="rb") as f:
                unique_user_ids = pickle.load(f)
            user_model = tf.keras.Sequential(
                [
                    tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),
                    # We add an additional embedding to account for unknown tokens.
                    tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),
                ]
            )

            with open(file=unique_movie_titles_path, mode="rb") as f:
                unique_movie_titles = pickle.load(f)
            movie_model = tf.keras.Sequential(
                [
                    tf.keras.layers.StringLookup(
                        vocabulary=unique_movie_titles, mask_token=None
                    ),
                    tf.keras.layers.Embedding(
                        len(unique_movie_titles) + 1, embedding_dimension
                    ),
                ]
            )
            metrics = tfrs.metrics.FactorizedTopK(candidates=movies.batch(128).map(movie_model))
            task = tfrs.tasks.Retrieval(metrics=metrics)

            class MovielensModel(tfrs.Model):
                def __init__(self, user_model, movie_model):
                    super().__init__()
                    self.movie_model: tf.keras.Model = movie_model
                    self.user_model: tf.keras.Model = user_model
                    self.task: tf.keras.layers.Layer = task

                def compute_loss(
                    self, features, training=False
                ):
                    # We pick out the user features and pass them into the user model.
                    user_embeddings = self.user_model(features["user_id"])
                    # And pick out the movie features and pass them into the movie model,
                    # getting embeddings back.
                    positive_movie_embeddings = self.movie_model(features["movie_title"])

                    # The task computes the loss and the metrics.
                    return self.task(user_embeddings, positive_movie_embeddings)

            model = MovielensModel(user_model, movie_model)
            model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))

            train = tf.data.Dataset.load(train_path, compression="GZIP")
            test = tf.data.Dataset.load(test_path, compression="GZIP")

            cached_train = train.shuffle(100_000).batch(8192).cache()
            cached_test = test.batch(4096).cache()

            model.fit(cached_train, epochs=3)
            model.evaluate(cached_test, return_dict=True)

            # Create a model that takes in raw query features, and
            index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)
            # recommends movies out of the entire movies dataset.
            index.index_from_dataset(
                tf.data.Dataset.zip(
                    (movies.batch(100), movies.batch(100).map(model.movie_model))
                )
            )

            # model.save_weights(f"{model_path}", save_format="h5")
            tf.saved_model.save(model, f"gs://{gcs_bucket_name}/basic_retrieval_model")

        import argparse
        _parser = argparse.ArgumentParser(prog='Build model', description='Function to build model.')
        _parser.add_argument("--movies", dest="movies_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-user-ids", dest="unique_user_ids_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-movie-titles", dest="unique_movie_titles_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train", dest="train_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test", dest="test_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--gcs-bucket-name", dest="gcs_bucket_name", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = build_model(**_parsed_args)
      image: eu.gcr.io/kubeflow-bg-experiment/recommender:latest
      imagePullPolicy: Always
    inputs:
      parameters:
      - {name: gcs_bucket_name}
      artifacts:
      - {name: prepare-dataset-movies, path: /tmp/inputs/movies/data}
      - {name: prepare-dataset-test, path: /tmp/inputs/test/data}
      - {name: prepare-dataset-train, path: /tmp/inputs/train/data}
      - {name: prepare-dataset-unique_movie_titles, path: /tmp/inputs/unique_movie_titles/data}
      - {name: prepare-dataset-unique_user_ids, path: /tmp/inputs/unique_user_ids/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "false"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Function
          to build model.", "implementation": {"container": {"args": ["--movies",
          {"inputPath": "movies"}, "--unique-user-ids", {"inputPath": "unique_user_ids"},
          "--unique-movie-titles", {"inputPath": "unique_movie_titles"}, "--train",
          {"inputPath": "train"}, "--test", {"inputPath": "test"}, "--gcs-bucket-name",
          {"inputValue": "gcs_bucket_name"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def build_model(\n    movies_path,\n    unique_user_ids_path,\n    unique_movie_titles_path,\n    train_path,\n    test_path,\n    gcs_bucket_name,\n):\n    \"\"\"Function
          to build model.\"\"\"\n    import pickle\n    from typing import Dict, Text\n\n    import
          tensorflow as tf\n    import tensorflow_recommenders as tfrs\n\n    embedding_dimension
          = 32\n\n    movies = tf.data.Dataset.load(movies_path, compression=\"GZIP\")\n\n    with
          open(file=unique_user_ids_path, mode=\"rb\") as f:\n        unique_user_ids
          = pickle.load(f)\n    user_model = tf.keras.Sequential(\n        [\n            tf.keras.layers.StringLookup(vocabulary=unique_user_ids,
          mask_token=None),\n            # We add an additional embedding to account
          for unknown tokens.\n            tf.keras.layers.Embedding(len(unique_user_ids)
          + 1, embedding_dimension),\n        ]\n    )\n\n    with open(file=unique_movie_titles_path,
          mode=\"rb\") as f:\n        unique_movie_titles = pickle.load(f)\n    movie_model
          = tf.keras.Sequential(\n        [\n            tf.keras.layers.StringLookup(\n                vocabulary=unique_movie_titles,
          mask_token=None\n            ),\n            tf.keras.layers.Embedding(\n                len(unique_movie_titles)
          + 1, embedding_dimension\n            ),\n        ]\n    )\n    metrics
          = tfrs.metrics.FactorizedTopK(candidates=movies.batch(128).map(movie_model))\n    task
          = tfrs.tasks.Retrieval(metrics=metrics)\n\n    class MovielensModel(tfrs.Model):\n        def
          __init__(self, user_model, movie_model):\n            super().__init__()\n            self.movie_model:
          tf.keras.Model = movie_model\n            self.user_model: tf.keras.Model
          = user_model\n            self.task: tf.keras.layers.Layer = task\n\n        def
          compute_loss(\n            self, features, training=False\n        ):\n            #
          We pick out the user features and pass them into the user model.\n            user_embeddings
          = self.user_model(features[\"user_id\"])\n            # And pick out the
          movie features and pass them into the movie model,\n            # getting
          embeddings back.\n            positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n\n            #
          The task computes the loss and the metrics.\n            return self.task(user_embeddings,
          positive_movie_embeddings)\n\n    model = MovielensModel(user_model, movie_model)\n    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n\n    train
          = tf.data.Dataset.load(train_path, compression=\"GZIP\")\n    test = tf.data.Dataset.load(test_path,
          compression=\"GZIP\")\n\n    cached_train = train.shuffle(100_000).batch(8192).cache()\n    cached_test
          = test.batch(4096).cache()\n\n    model.fit(cached_train, epochs=3)\n    model.evaluate(cached_test,
          return_dict=True)\n\n    # Create a model that takes in raw query features,
          and\n    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n    #
          recommends movies out of the entire movies dataset.\n    index.index_from_dataset(\n        tf.data.Dataset.zip(\n            (movies.batch(100),
          movies.batch(100).map(model.movie_model))\n        )\n    )\n\n    # model.save_weights(f\"{model_path}\",
          save_format=\"h5\")\n    tf.saved_model.save(model, f\"gs://{gcs_bucket_name}/basic_retrieval_model\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Build model'', description=''Function
          to build model.'')\n_parser.add_argument(\"--movies\", dest=\"movies_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-user-ids\",
          dest=\"unique_user_ids_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-movie-titles\",
          dest=\"unique_movie_titles_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train\",
          dest=\"train_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test\",
          dest=\"test_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--gcs-bucket-name\",
          dest=\"gcs_bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = build_model(**_parsed_args)\n"],
          "image": "eu.gcr.io/kubeflow-bg-experiment/recommender:latest"}}, "inputs":
          [{"name": "movies"}, {"name": "unique_user_ids"}, {"name": "unique_movie_titles"},
          {"name": "train"}, {"name": "test"}, {"name": "gcs_bucket_name", "type":
          "String"}], "name": "Build model"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"gcs_bucket_name": "{{inputs.parameters.gcs_bucket_name}}"}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: prepare-dataset
    container:
      args: [--movies, /tmp/outputs/movies/data, --train, /tmp/outputs/train/data,
        --test, /tmp/outputs/test/data, --unique-movie-titles, /tmp/outputs/unique_movie_titles/data,
        --unique-user-ids, /tmp/outputs/unique_user_ids/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_dataset(
            movies_path,
            train_path,
            test_path,
            unique_movie_titles_path,
            unique_user_ids_path,
        ):
            """Function to prepare dataset."""
            import pickle

            import numpy as np
            import tensorflow as tf
            import tensorflow_datasets as tfds

            # Ratings data.
            ratings = tfds.load("movielens/100k-ratings", split="train")
            # Features of all the available movies.
            movies = tfds.load("movielens/100k-movies", split="train")

            ratings = ratings.map(
                lambda x: {
                    "movie_title": x["movie_title"],
                    "user_id": x["user_id"],
                }
            )
            movies = movies.map(lambda x: x["movie_title"])

            movies.save(movies_path, compression="GZIP")

            tf.random.set_seed(42)
            shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)

            train = shuffled.take(80_000)
            test = shuffled.skip(80_000).take(20_000)

            train.save(train_path, compression="GZIP")
            test.save(test_path, compression="GZIP")

            movie_titles = movies.batch(1_000)
            user_ids = ratings.batch(1_000_000).map(lambda x: x["user_id"])

            unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))
            unique_user_ids = np.unique(np.concatenate(list(user_ids)))

            with open(file=unique_movie_titles_path, mode="wb") as f:
                pickle.dump(unique_movie_titles, f)
            with open(file=unique_user_ids_path, mode="wb") as f:
                pickle.dump(unique_user_ids, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare dataset', description='Function to prepare dataset.')
        _parser.add_argument("--movies", dest="movies_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train", dest="train_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test", dest="test_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-movie-titles", dest="unique_movie_titles_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-user-ids", dest="unique_user_ids_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_dataset(**_parsed_args)
      image: eu.gcr.io/kubeflow-bg-experiment/recommender:latest
      imagePullPolicy: Always
    outputs:
      artifacts:
      - {name: prepare-dataset-movies, path: /tmp/outputs/movies/data}
      - {name: prepare-dataset-test, path: /tmp/outputs/test/data}
      - {name: prepare-dataset-train, path: /tmp/outputs/train/data}
      - {name: prepare-dataset-unique_movie_titles, path: /tmp/outputs/unique_movie_titles/data}
      - {name: prepare-dataset-unique_user_ids, path: /tmp/outputs/unique_user_ids/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "false"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Function
          to prepare dataset.", "implementation": {"container": {"args": ["--movies",
          {"outputPath": "movies"}, "--train", {"outputPath": "train"}, "--test",
          {"outputPath": "test"}, "--unique-movie-titles", {"outputPath": "unique_movie_titles"},
          "--unique-user-ids", {"outputPath": "unique_user_ids"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_dataset(\n    movies_path,\n    train_path,\n    test_path,\n    unique_movie_titles_path,\n    unique_user_ids_path,\n):\n    \"\"\"Function
          to prepare dataset.\"\"\"\n    import pickle\n\n    import numpy as np\n    import
          tensorflow as tf\n    import tensorflow_datasets as tfds\n\n    # Ratings
          data.\n    ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n    #
          Features of all the available movies.\n    movies = tfds.load(\"movielens/100k-movies\",
          split=\"train\")\n\n    ratings = ratings.map(\n        lambda x: {\n            \"movie_title\":
          x[\"movie_title\"],\n            \"user_id\": x[\"user_id\"],\n        }\n    )\n    movies
          = movies.map(lambda x: x[\"movie_title\"])\n\n    movies.save(movies_path,
          compression=\"GZIP\")\n\n    tf.random.set_seed(42)\n    shuffled = ratings.shuffle(100_000,
          seed=42, reshuffle_each_iteration=False)\n\n    train = shuffled.take(80_000)\n    test
          = shuffled.skip(80_000).take(20_000)\n\n    train.save(train_path, compression=\"GZIP\")\n    test.save(test_path,
          compression=\"GZIP\")\n\n    movie_titles = movies.batch(1_000)\n    user_ids
          = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n\n    unique_movie_titles
          = np.unique(np.concatenate(list(movie_titles)))\n    unique_user_ids = np.unique(np.concatenate(list(user_ids)))\n\n    with
          open(file=unique_movie_titles_path, mode=\"wb\") as f:\n        pickle.dump(unique_movie_titles,
          f)\n    with open(file=unique_user_ids_path, mode=\"wb\") as f:\n        pickle.dump(unique_user_ids,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          dataset'', description=''Function to prepare dataset.'')\n_parser.add_argument(\"--movies\",
          dest=\"movies_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--train\", dest=\"train_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test\",
          dest=\"test_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-movie-titles\",
          dest=\"unique_movie_titles_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-user-ids\",
          dest=\"unique_user_ids_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_dataset(**_parsed_args)\n"], "image": "eu.gcr.io/kubeflow-bg-experiment/recommender:latest"}},
          "name": "Prepare dataset", "outputs": [{"name": "movies"}, {"name": "train"},
          {"name": "test"}, {"name": "unique_movie_titles"}, {"name": "unique_user_ids"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  arguments:
    parameters:
    - {name: gcs_bucket_name}
  serviceAccountName: pipeline-runner
