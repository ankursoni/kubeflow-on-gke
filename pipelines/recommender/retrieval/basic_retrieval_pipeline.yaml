apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: basic-retrieval-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19, pipelines.kubeflow.org/pipeline_compilation_time: '2023-03-07T21:02:34.116240',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Basic retrieval pipeline",
      "inputs": [{"name": "gcs_bucket_name"}], "name": "Basic retrieval pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.19}
spec:
  entrypoint: basic-retrieval-pipeline
  templates:
  - name: basic-retrieval-pipeline
    inputs:
      parameters:
      - {name: gcs_bucket_name}
    dag:
      tasks:
      - name: build-model
        template: build-model
        dependencies: [prepare-dataset]
        arguments:
          artifacts:
          - {name: prepare-dataset-movies, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-movies}}'}
          - {name: prepare-dataset-test, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-test}}'}
          - {name: prepare-dataset-train, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-train}}'}
          - {name: prepare-dataset-unique_movie_titles, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-unique_movie_titles}}'}
          - {name: prepare-dataset-unique_user_ids, from: '{{tasks.prepare-dataset.outputs.artifacts.prepare-dataset-unique_user_ids}}'}
      - {name: prepare-dataset, template: prepare-dataset}
      - name: trained-files-to-gcs
        template: trained-files-to-gcs
        dependencies: [build-model]
        arguments:
          parameters:
          - {name: gcs_bucket_name, value: '{{inputs.parameters.gcs_bucket_name}}'}
          artifacts:
          - {name: build-model-model, from: '{{tasks.build-model.outputs.artifacts.build-model-model}}'}
  - name: build-model
    container:
      args: [--movies, /tmp/inputs/movies/data, --unique-user-ids, /tmp/inputs/unique_user_ids/data,
        --unique-movie-titles, /tmp/inputs/unique_movie_titles/data, --train, /tmp/inputs/train/data,
        --test, /tmp/inputs/test/data, --model, /tmp/outputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def build_model(
            movies_path,
            unique_user_ids_path,
            unique_movie_titles_path,
            train_path,
            test_path,
            model_path,
        ):
            """Function to build model."""
            import pickle
            from typing import Dict, Text

            import tensorflow as tf
            import tensorflow_recommenders as tfrs

            embedding_dimension = 32

            movies = tf.data.Dataset.load(movies_path, compression="GZIP")

            with open(file=unique_user_ids_path, mode="rb") as f:
                unique_user_ids = pickle.load(f)
            user_model = tf.keras.Sequential(
                [
                    tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),
                    # We add an additional embedding to account for unknown tokens.
                    tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension),
                ]
            )

            with open(file=unique_movie_titles_path, mode="rb") as f:
                unique_movie_titles = pickle.load(f)
            movie_model = tf.keras.Sequential(
                [
                    tf.keras.layers.StringLookup(
                        vocabulary=unique_movie_titles, mask_token=None
                    ),
                    tf.keras.layers.Embedding(
                        len(unique_movie_titles) + 1, embedding_dimension
                    ),
                ]
            )
            metrics = tfrs.metrics.FactorizedTopK(candidates=movies.batch(128).map(movie_model))
            task = tfrs.tasks.Retrieval(metrics=metrics)

            class MovielensModel(tfrs.Model):
                def __init__(self, user_model, movie_model):
                    super().__init__()
                    self.movie_model: tf.keras.Model = movie_model
                    self.user_model: tf.keras.Model = user_model
                    self.task: tf.keras.layers.Layer = task

                def compute_loss(
                    self, features, training=False
                ):
                    # We pick out the user features and pass them into the user model.
                    user_embeddings = self.user_model(features["user_id"])
                    # And pick out the movie features and pass them into the movie model,
                    # getting embeddings back.
                    positive_movie_embeddings = self.movie_model(features["movie_title"])

                    # The task computes the loss and the metrics.
                    return self.task(user_embeddings, positive_movie_embeddings)

            model = MovielensModel(user_model, movie_model)
            model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))

            train = tf.data.Dataset.load(train_path, compression="GZIP")
            test = tf.data.Dataset.load(test_path, compression="GZIP")

            cached_train = train.shuffle(100_000).batch(8192).cache()
            cached_test = test.batch(4096).cache()

            model.fit(cached_train, epochs=3)
            model.evaluate(cached_test, return_dict=True)

            # Create a model that takes in raw query features, and
            index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)
            # recommends movies out of the entire movies dataset.
            index.index_from_dataset(
                tf.data.Dataset.zip(
                    (movies.batch(100), movies.batch(100).map(model.movie_model))
                )
            )

            model.save(f"{model_path}/basic_retrieval_model.h5")

        import argparse
        _parser = argparse.ArgumentParser(prog='Build model', description='Function to build model.')
        _parser.add_argument("--movies", dest="movies_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-user-ids", dest="unique_user_ids_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-movie-titles", dest="unique_movie_titles_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train", dest="train_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test", dest="test_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = build_model(**_parsed_args)
      image: eu.gcr.io/kubeflow-bg-experiment/recommender:latest
      imagePullPolicy: Always
    inputs:
      artifacts:
      - {name: prepare-dataset-movies, path: /tmp/inputs/movies/data}
      - {name: prepare-dataset-test, path: /tmp/inputs/test/data}
      - {name: prepare-dataset-train, path: /tmp/inputs/train/data}
      - {name: prepare-dataset-unique_movie_titles, path: /tmp/inputs/unique_movie_titles/data}
      - {name: prepare-dataset-unique_user_ids, path: /tmp/inputs/unique_user_ids/data}
    outputs:
      artifacts:
      - {name: build-model-model, path: /tmp/outputs/model/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "false"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Function
          to build model.", "implementation": {"container": {"args": ["--movies",
          {"inputPath": "movies"}, "--unique-user-ids", {"inputPath": "unique_user_ids"},
          "--unique-movie-titles", {"inputPath": "unique_movie_titles"}, "--train",
          {"inputPath": "train"}, "--test", {"inputPath": "test"}, "--model", {"outputPath":
          "model"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          _make_parent_dirs_and_return_path(file_path: str):\n    import os\n    os.makedirs(os.path.dirname(file_path),
          exist_ok=True)\n    return file_path\n\ndef build_model(\n    movies_path,\n    unique_user_ids_path,\n    unique_movie_titles_path,\n    train_path,\n    test_path,\n    model_path,\n):\n    \"\"\"Function
          to build model.\"\"\"\n    import pickle\n    from typing import Dict, Text\n\n    import
          tensorflow as tf\n    import tensorflow_recommenders as tfrs\n\n    embedding_dimension
          = 32\n\n    movies = tf.data.Dataset.load(movies_path, compression=\"GZIP\")\n\n    with
          open(file=unique_user_ids_path, mode=\"rb\") as f:\n        unique_user_ids
          = pickle.load(f)\n    user_model = tf.keras.Sequential(\n        [\n            tf.keras.layers.StringLookup(vocabulary=unique_user_ids,
          mask_token=None),\n            # We add an additional embedding to account
          for unknown tokens.\n            tf.keras.layers.Embedding(len(unique_user_ids)
          + 1, embedding_dimension),\n        ]\n    )\n\n    with open(file=unique_movie_titles_path,
          mode=\"rb\") as f:\n        unique_movie_titles = pickle.load(f)\n    movie_model
          = tf.keras.Sequential(\n        [\n            tf.keras.layers.StringLookup(\n                vocabulary=unique_movie_titles,
          mask_token=None\n            ),\n            tf.keras.layers.Embedding(\n                len(unique_movie_titles)
          + 1, embedding_dimension\n            ),\n        ]\n    )\n    metrics
          = tfrs.metrics.FactorizedTopK(candidates=movies.batch(128).map(movie_model))\n    task
          = tfrs.tasks.Retrieval(metrics=metrics)\n\n    class MovielensModel(tfrs.Model):\n        def
          __init__(self, user_model, movie_model):\n            super().__init__()\n            self.movie_model:
          tf.keras.Model = movie_model\n            self.user_model: tf.keras.Model
          = user_model\n            self.task: tf.keras.layers.Layer = task\n\n        def
          compute_loss(\n            self, features, training=False\n        ):\n            #
          We pick out the user features and pass them into the user model.\n            user_embeddings
          = self.user_model(features[\"user_id\"])\n            # And pick out the
          movie features and pass them into the movie model,\n            # getting
          embeddings back.\n            positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n\n            #
          The task computes the loss and the metrics.\n            return self.task(user_embeddings,
          positive_movie_embeddings)\n\n    model = MovielensModel(user_model, movie_model)\n    model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n\n    train
          = tf.data.Dataset.load(train_path, compression=\"GZIP\")\n    test = tf.data.Dataset.load(test_path,
          compression=\"GZIP\")\n\n    cached_train = train.shuffle(100_000).batch(8192).cache()\n    cached_test
          = test.batch(4096).cache()\n\n    model.fit(cached_train, epochs=3)\n    model.evaluate(cached_test,
          return_dict=True)\n\n    # Create a model that takes in raw query features,
          and\n    index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n    #
          recommends movies out of the entire movies dataset.\n    index.index_from_dataset(\n        tf.data.Dataset.zip(\n            (movies.batch(100),
          movies.batch(100).map(model.movie_model))\n        )\n    )\n\n    model.save(f\"{model_path}/basic_retrieval_model.h5\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Build model'', description=''Function
          to build model.'')\n_parser.add_argument(\"--movies\", dest=\"movies_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-user-ids\",
          dest=\"unique_user_ids_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-movie-titles\",
          dest=\"unique_movie_titles_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train\",
          dest=\"train_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test\",
          dest=\"test_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = build_model(**_parsed_args)\n"], "image": "eu.gcr.io/kubeflow-bg-experiment/recommender:latest"}},
          "inputs": [{"name": "movies"}, {"name": "unique_user_ids"}, {"name": "unique_movie_titles"},
          {"name": "train"}, {"name": "test"}], "name": "Build model", "outputs":
          [{"name": "model"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: prepare-dataset
    container:
      args: [--movies, /tmp/outputs/movies/data, --train, /tmp/outputs/train/data,
        --test, /tmp/outputs/test/data, --unique-movie-titles, /tmp/outputs/unique_movie_titles/data,
        --unique-user-ids, /tmp/outputs/unique_user_ids/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def prepare_dataset(
            movies_path,
            train_path,
            test_path,
            unique_movie_titles_path,
            unique_user_ids_path,
        ):
            """Function to prepare dataset."""
            import pickle

            import numpy as np
            import tensorflow as tf
            import tensorflow_datasets as tfds

            # Ratings data.
            ratings = tfds.load("movielens/100k-ratings", split="train")
            # Features of all the available movies.
            movies = tfds.load("movielens/100k-movies", split="train")

            movies.save(movies_path, compression="GZIP")

            ratings = ratings.map(
                lambda x: {
                    "movie_title": x["movie_title"],
                    "user_id": x["user_id"],
                }
            )
            movies = movies.map(lambda x: x["movie_title"])

            tf.random.set_seed(42)
            shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)

            train = shuffled.take(80_000)
            test = shuffled.skip(80_000).take(20_000)

            train.save(train_path, compression="GZIP")
            test.save(test_path, compression="GZIP")

            movie_titles = movies.batch(1_000)
            user_ids = ratings.batch(1_000_000).map(lambda x: x["user_id"])

            unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))
            unique_user_ids = np.unique(np.concatenate(list(user_ids)))

            with open(file=unique_movie_titles_path, mode="wb") as f:
                pickle.dump(unique_movie_titles, f)
            with open(file=unique_user_ids_path, mode="wb") as f:
                pickle.dump(unique_user_ids, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Prepare dataset', description='Function to prepare dataset.')
        _parser.add_argument("--movies", dest="movies_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train", dest="train_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test", dest="test_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-movie-titles", dest="unique_movie_titles_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--unique-user-ids", dest="unique_user_ids_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = prepare_dataset(**_parsed_args)
      image: eu.gcr.io/kubeflow-bg-experiment/recommender:latest
      imagePullPolicy: Always
    outputs:
      artifacts:
      - {name: prepare-dataset-movies, path: /tmp/outputs/movies/data}
      - {name: prepare-dataset-test, path: /tmp/outputs/test/data}
      - {name: prepare-dataset-train, path: /tmp/outputs/train/data}
      - {name: prepare-dataset-unique_movie_titles, path: /tmp/outputs/unique_movie_titles/data}
      - {name: prepare-dataset-unique_user_ids, path: /tmp/outputs/unique_user_ids/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "false"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Function
          to prepare dataset.", "implementation": {"container": {"args": ["--movies",
          {"outputPath": "movies"}, "--train", {"outputPath": "train"}, "--test",
          {"outputPath": "test"}, "--unique-movie-titles", {"outputPath": "unique_movie_titles"},
          "--unique-user-ids", {"outputPath": "unique_user_ids"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef prepare_dataset(\n    movies_path,\n    train_path,\n    test_path,\n    unique_movie_titles_path,\n    unique_user_ids_path,\n):\n    \"\"\"Function
          to prepare dataset.\"\"\"\n    import pickle\n\n    import numpy as np\n    import
          tensorflow as tf\n    import tensorflow_datasets as tfds\n\n    # Ratings
          data.\n    ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n    #
          Features of all the available movies.\n    movies = tfds.load(\"movielens/100k-movies\",
          split=\"train\")\n\n    movies.save(movies_path, compression=\"GZIP\")\n\n    ratings
          = ratings.map(\n        lambda x: {\n            \"movie_title\": x[\"movie_title\"],\n            \"user_id\":
          x[\"user_id\"],\n        }\n    )\n    movies = movies.map(lambda x: x[\"movie_title\"])\n\n    tf.random.set_seed(42)\n    shuffled
          = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n\n    train
          = shuffled.take(80_000)\n    test = shuffled.skip(80_000).take(20_000)\n\n    train.save(train_path,
          compression=\"GZIP\")\n    test.save(test_path, compression=\"GZIP\")\n\n    movie_titles
          = movies.batch(1_000)\n    user_ids = ratings.batch(1_000_000).map(lambda
          x: x[\"user_id\"])\n\n    unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n    unique_user_ids
          = np.unique(np.concatenate(list(user_ids)))\n\n    with open(file=unique_movie_titles_path,
          mode=\"wb\") as f:\n        pickle.dump(unique_movie_titles, f)\n    with
          open(file=unique_user_ids_path, mode=\"wb\") as f:\n        pickle.dump(unique_user_ids,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Prepare
          dataset'', description=''Function to prepare dataset.'')\n_parser.add_argument(\"--movies\",
          dest=\"movies_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--train\", dest=\"train_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test\",
          dest=\"test_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-movie-titles\",
          dest=\"unique_movie_titles_path\", type=_make_parent_dirs_and_return_path,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--unique-user-ids\",
          dest=\"unique_user_ids_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = prepare_dataset(**_parsed_args)\n"], "image": "eu.gcr.io/kubeflow-bg-experiment/recommender:latest"}},
          "name": "Prepare dataset", "outputs": [{"name": "movies"}, {"name": "train"},
          {"name": "test"}, {"name": "unique_movie_titles"}, {"name": "unique_user_ids"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: trained-files-to-gcs
    container:
      args: [--gcs-bucket-name, '{{inputs.parameters.gcs_bucket_name}}', --model,
        /tmp/inputs/model/data, '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def trained_files_to_gcs(
            # test_result: bool,
            gcs_bucket_name,
            model_path,
        ):
            """Function to upload training output to gcs bucket."""
            from google.cloud import storage

            # if not test_result:
            #     return

            client = storage.Client()
            bucket = client.bucket(gcs_bucket_name)

            with open(file=f"{model_path}/basic_retrieval_model.h5", mode="rb") as file:
                blob = bucket.blob("basic_retrieval_model.h5")
                blob.upload_from_file(file, content_type="bytes")

            return True

        def _serialize_bool(bool_value: bool) -> str:
            if isinstance(bool_value, str):
                return bool_value
            if not isinstance(bool_value, bool):
                raise TypeError('Value "{}" has type "{}" instead of bool.'.format(
                    str(bool_value), str(type(bool_value))))
            return str(bool_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Trained files to gcs', description='Function to upload training output to gcs bucket.')
        _parser.add_argument("--gcs-bucket-name", dest="gcs_bucket_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = trained_files_to_gcs(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_bool,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: eu.gcr.io/kubeflow-bg-experiment/recommender:latest
      imagePullPolicy: Always
    inputs:
      parameters:
      - {name: gcs_bucket_name}
      artifacts:
      - {name: build-model-model, path: /tmp/inputs/model/data}
    outputs:
      artifacts:
      - {name: trained-files-to-gcs-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.19
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "false"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Function
          to upload training output to gcs bucket.", "implementation": {"container":
          {"args": ["--gcs-bucket-name", {"inputValue": "gcs_bucket_name"}, "--model",
          {"inputPath": "model"}, "----output-paths", {"outputPath": "Output"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def trained_files_to_gcs(\n    # test_result:
          bool,\n    gcs_bucket_name,\n    model_path,\n):\n    \"\"\"Function to
          upload training output to gcs bucket.\"\"\"\n    from google.cloud import
          storage\n\n    # if not test_result:\n    #     return\n\n    client = storage.Client()\n    bucket
          = client.bucket(gcs_bucket_name)\n\n    with open(file=f\"{model_path}/basic_retrieval_model.h5\",
          mode=\"rb\") as file:\n        blob = bucket.blob(\"basic_retrieval_model.h5\")\n        blob.upload_from_file(file,
          content_type=\"bytes\")\n\n    return True\n\ndef _serialize_bool(bool_value:
          bool) -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(\n            str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Trained files to gcs'', description=''Function
          to upload training output to gcs bucket.'')\n_parser.add_argument(\"--gcs-bucket-name\",
          dest=\"gcs_bucket_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = trained_files_to_gcs(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_bool,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "eu.gcr.io/kubeflow-bg-experiment/recommender:latest"}}, "inputs":
          [{"name": "gcs_bucket_name", "type": "String"}, {"name": "model"}], "name":
          "Trained files to gcs", "outputs": [{"name": "Output", "type": "Boolean"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"gcs_bucket_name":
          "{{inputs.parameters.gcs_bucket_name}}"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  arguments:
    parameters:
    - {name: gcs_bucket_name}
  serviceAccountName: pipeline-runner
